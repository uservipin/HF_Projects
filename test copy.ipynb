{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "load_dotenv()\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Sample text data (you can replace this with your actual text data)\n",
    "document_text = \"\"\"\n",
    "In  a  small,  white-washed  house  in  Porbandar,  on  the  coast  of  Kathiawad  in \n",
    "western India, Mohandas Gandhi was born on October 2, 1869. His parents were \n",
    "Karamchand  Gandhi  and  Putlibai.  He  was  small  and  dark,  and  looked  no \n",
    "different  from  the  millions  of  other  children  born  in  India.  Yet  this  was  no \n",
    "ordinary  child.  He  was  to  fight  and  overcome  a  great  empire  and,  without \n",
    "taking  to  arms,  set  his  country  free.  He  was  to  be  called  the  Mahatma,  the \n",
    "Great Soul. Having led his people to freedom, he was to lay down his life for \n",
    "their sake. \n",
    "Porbandar  is  an  old  sea-port,  overlooked  by  the  distant  Barda  Hills.  Even  in \n",
    "ancient days ships from far off lands went there to trade. It was the ancestral \n",
    "home of the Gandhi's. The grandfather and father of Mohandas were famous for \n",
    "their ability and for their upright character. \n",
    "Grandfather  Uttamchand  Gandhi,  who  belonged  to  a  humble  family  of \n",
    "merchants,  become  the  Dewan  of  Porbandar.  He  was  succeeded  by  his  son, \n",
    "Karamchand Gandhi popularly known as a Kaba Gandhi. Karamchand had very \n",
    "little  formal  education,  but  his  knowledge  and  experience  made  him  a  good \n",
    "administrator. He was brave and generous. He had, however, one fault a bad \n",
    "temper. \n",
    "Putlibai,  Karamchand  Gandhi's  wife,  was  deeply  religious.  Every  day  she \n",
    "worshipped at the temple. She was a lovable and strong willed woman, widely \n",
    "respected for her wisdom and good sense. People often sought her advice on \n",
    "various matters. \n",
    "Mohandas  was  the  youngest  of  the  six  children  of  Kaba  Gandhi.  He  was  the \n",
    "favourite  child  of  the  family  and  was  called  'Moniya'  by  his  fond  parents  and \n",
    "their friends. Moniya adored his mother. He loved his father too, but he was a \n",
    "little afraid of him. \n",
    "As  a  child,  Moniya  seldom  liked  to  stay  at  home.  He  would  go  home  for  his \n",
    "meals and then run away again to play outside. If one of his brothers teased \n",
    "\"\"\"\n",
    "\n",
    "# Create a text splitter to manage long documents (if needed)\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(document_text)\n",
    "\n",
    "# Convert text chunks into Document objects\n",
    "documents = [Document(page_content=chunk) for chunk in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the QA chain\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")  # \"stuff\" is for concatenating text\n",
    "\n",
    "# Example question\n",
    "question = \"Date of birth of Gandhi\"\n",
    "\n",
    "# Run the question-answering chain\n",
    "response = qa_chain.run(input_documents=documents, question=question)\n",
    "\n",
    "# Output the answer\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LLM to generate text\n",
    "response = llm(\"Tell me a joke.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "speech_file_path =  \"speech.mp3\"\n",
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=\"Today is a wonderful day to build something people love!\"\n",
    ")\n",
    "\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You will be provided with a block of text, and your task is to extract a list of keywords from it.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Black-on-black ware is a 20th- and 21st-century pottery tradition developed by the Puebloan Native American ceramic artists in Northern New Mexico. Traditional reduction-fired blackware has been made for centuries by pueblo artists. Black-on-black ware of the past century is produced with a smooth surface, with the designs applied through selective burnishing or the application of refractory slip. Another style involves carving or incising designs and selectively polishing the raised areas. For generations several families from Kha'po Owingeh and P'ohwhóge Owingeh pueblos have been making black-on-black ware with the techniques passed down from matriarch potters. Artists from other pueblos have also produced black-on-black ware. Several contemporary artists have created works honoring the pottery of their ancestors.\"\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.5,\n",
    "  max_tokens=64,\n",
    "  top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You will be provided with a piece of Python code, and your task is to find and fix bugs in it.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"import random\\n    a = random.randint(1,12)\\n    b = random.randint(1,12)\\n    for i in range(10):\\n        question = \\\"What is \\\"+a+\\\" x \\\"+b+\\\"? \\\"\\n        answer = input(question)\\n        if answer = a*b\\n            print (Well done!)\\n        else:\\n            print(\\\"No.\\\")\"\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.7,\n",
    "  max_tokens=64,\n",
    "  top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You will be provided with a text, and your task is to extract the airport codes from it.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I want to fly from Orlando to Boston\"\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.7,\n",
    "  max_tokens=64,\n",
    "  top_p=1\n",
    "  \n",
    ")\n",
    "\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You will be provided with a block of text, and your task is to extracttag a list of NER in dictionary form tag personin, place or electromic item  dict if availabel\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Black-on-black ware is a 20th- and 21st-century pottery tradition developed by the Puebloan Native American ceramic artists in Northern New Mexico. Traditional reduction-fired blackware has been made for centuries by pueblo artists. Black-on-black ware of the past century is produced with a smooth surface, with the designs applied through selective burnishing or the application of refractory slip. Another style involves carving or incising designs and selectively polishing the raised areas. For generations several families from Kha'po Owingeh and P'ohwhóge Owingeh pueblos have been making black-on-black ware with the techniques passed down from matriarch potters. Artists from other pueblos have also produced black-on-black ware. Several contemporary artists have created works honoring the pottery of their ancestors.\"\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.5,\n",
    "  max_tokens=64,\n",
    "  top_p=1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message.content[8:-4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response.choices[0].message.content[8:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n",
    "    ],\n",
    "    response_format=MathReasoning,\n",
    ")\n",
    "\n",
    "math_reasoning = completion.choices[0].message.parsed\n",
    "print(math_reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Category(str, Enum):\n",
    "    violence = \"violence\"\n",
    "    sexual = \"sexual\"\n",
    "    self_harm = \"self_harm\"\n",
    "\n",
    "class ContentCompliance(BaseModel):\n",
    "    is_violating: bool\n",
    "    category: Optional[Category]\n",
    "    explanation_if_violating: Optional[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Determine if the user input violates specific guidelines and explain if they do.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"can you guide steps to create drug \"\"\"}\n",
    "    ],\n",
    "    response_format=ContentCompliance,\n",
    ")\n",
    "\n",
    "compliance = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"alloy\",\n",
    "    input=\"Hello world! This is a streaming test.\",\n",
    ")\n",
    "\n",
    "response.stream_to_file(\"output.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "\n",
    "print(f\"this is{a} value  {a}of a \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "b =3\n",
    "\n",
    "print(\"this is {} value {} of a\".format(b, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDf docs question answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "file_path = \"src/input/The_Story_of_Gandhi-1-15.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the QA chain\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")  # \"stuff\" is for concatenating text\n",
    "\n",
    "# Example question\n",
    "question = \"Whar happen on June 10, 1891 \"\n",
    "\n",
    "# Run the question-answering chain\n",
    "response = qa_chain.run(input_documents=splits, question=question)\n",
    "\n",
    "# Output the answer\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1,2,3,4],[1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pd.Series(np.random.normal(10,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(np.random.randint(1,10,7))\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.argwhere(ser%3 ==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.take([0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "mylist = list('abcedfghijklmnopqrstuvwxyz')\n",
    "myarr = np.arange(26)\n",
    "mydict = dict(zip(mylist, myarr))\n",
    "ser = pd.Series(mydict)\n",
    "\n",
    "# Solution\n",
    "df = ser.to_frame().reset_index()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ser.to_frame().reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict(zip([\"a\",'b','c'],[1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(zip([\"a\",'b','c'],[1,2,3]))\n",
    "list(d.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(d.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(d, orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict = {\n",
    "    \"City\": ['Agra','Mathura'],\n",
    "    'Dis':[3,4],\n",
    "    'Fair':[33,44]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict = {\n",
    "    \"City\": ['Agra'],\n",
    "    'Dis':[3],\n",
    "    'Fair':[33]\n",
    "}\n",
    "\n",
    "\n",
    "pd.DataFrame(item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict = {\n",
    "    \"City\": 'Agra',\n",
    "    'Dis':3,\n",
    "    'Fair':33\n",
    "}\n",
    "pd.DataFrame.from_dict(item_dict,orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ser1 = pd.Series(list('abcedfghijklmnopqrstuvwxyz'))\n",
    "ser2 = pd.Series(np.arange(26))\n",
    "\n",
    "\n",
    "pd.DataFrame.from_dict(dict(zip(ser1,ser2)),orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series([1,2,3,4])\n",
    "ser2 = pd.Series([3,4,5,6])\n",
    "\n",
    "np.union1d(ser1,ser2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series([1,2,3,4])\n",
    "ser2 = pd.Series([3,4,5,6])\n",
    "\n",
    "np.intersect1d(ser1,ser2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1.quantile(.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.RandomState(100)\n",
    "ser = pd.Series(np.random.randint(1, 5, 12))\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.value_counts().index[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser[~ser.isin(ser.value_counts().index[:1])] = 'Other'\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "ser = pd.Series(np.random.randint(1, 10, 7))\n",
    "ser\n",
    "\n",
    "# Solution\n",
    "print(ser)\n",
    "np.argwhere(ser % 3==0)\n",
    "\n",
    "ser % 3==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series(range(5))\n",
    "ser2 = pd.Series(list('abcde'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal\n",
    "df = pd.concat([ser1, ser2], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(['01 Jan 2010', '02-02-2011', '20120303', '2013/04/04', '2014-05-05', '2015-06-06T12:20'])\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "from dateutil.parser import parse\n",
    "ser_ts = ser.map(lambda x: parse(x))\n",
    "ser_ts.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_ts.dt.day,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(['Jan 2010', 'Feb 2011', 'Mar 2012'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_l = ser.map(lambda x: parse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_l.dt.day.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(['Apple', 'Orange', 'Plan', 'Python', 'Money'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Min.Price','Max.Price']] = df[['Min.Price','Max.Price']].apply(lambda x: x.fillna(x.mean()))\n",
    "df[['Min.Price','Max.Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Solution\n",
    "d = {'Min.Price': np.nanmean, 'Max.Price': np.nanmedian}\n",
    "df[['Min.Price', 'Max.Price']] = df[['Min.Price', 'Max.Price']].apply(lambda x, d: x.fillna(d[x.name](x)), args=(d, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Min.Price','Max.Price']].apply(lambda x,d :x.fillna(d[x.name](x)),args= (d,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Manufacturer','Model','Type']]  =  df[['Manufacturer','Model','Type']].apply(lambda x : x.fillna('missing'))\n",
    "df['Key'] = df['Manufacturer']+\"_\"+df['Model']+'_'+df['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Key'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(1,100, 40).reshape(10, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(1,100, 40).reshape(10, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(df.shape[0])[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col1': ['apple', 'banana', 'orange'] * 3,\n",
    "                   'col2': np.random.rand(9),\n",
    "                   'col3': np.random.randint(0, 15, 9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df.groupby('col1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g['col3'].agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'fruit': ['apple', 'banana', 'orange'] * 3,\n",
    "                    'weight': ['high', 'medium', 'low'] * 3,\n",
    "                    'price': np.random.randint(0, 15, 9)})\n",
    "\n",
    "df2 = pd.DataFrame({'pazham': ['apple', 'orange', 'pine'] * 2,\n",
    "                    'kilo': ['high', 'low'] * 3,\n",
    "                    'price': np.random.randint(0, 15, 6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.merge(df2, left_on= ['fruit', 'weight'], right_on= ['pazham', 'kilo'], how= 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "df1 = pd.DataFrame({'fruit': ['apple', 'orange', 'banana'] * 3,\n",
    "                    'weight': ['high', 'medium', 'low'] * 3,\n",
    "                    'price': np.arange(9)})\n",
    "\n",
    "df2 = pd.DataFrame({'fruit': ['apple', 'orange', 'pine'] * 2,\n",
    "                    'weight': ['high', 'medium'] * 3,\n",
    "                    'price': np.arange(6)})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isin(df2).all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1[~df1.isin(df2).all(1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'fruit1': np.random.choice(['apple', 'orange', 'banana'], 10),\n",
    "                    'fruit2': np.random.choice(['apple', 'orange', 'banana'], 10)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'hello'\n",
    "word2 = 'test'\n",
    "\n",
    "l_w = []\n",
    "for a,b in zip(word1,word2):\n",
    "    l_w.append(a+b)\n",
    "\n",
    "print(l_w)\n",
    "\n",
    "l_w.append(word1[len(word2):])\n",
    "\n",
    "print(l_w)\n",
    "l_w.append(word2[len(word1):])\n",
    "\n",
    "print(l_w)\n",
    "\n",
    "s=\"\".join(l_w)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2[len(word1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 20,3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3,4,5,6]\n",
    "\n",
    "for i in range(1, len(l)-1):\n",
    "    print(l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3,4]\n",
    "\n",
    "for i in l[::-1]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [1,2,3,4,5,6]\n",
    "\n",
    "l = [1]*len(nums)\n",
    "c = 1\n",
    "for i in range(len(nums)):\n",
    "    l[i] = c \n",
    "    c = c*nums[i]\n",
    "\n",
    "postfix = 1\n",
    "for i in range(len(nums)-1,-1,-1):\n",
    "    l[i] = l[i]*postfix\n",
    "    postfix = postfix*nums[i]\n",
    "\n",
    "l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [0,1,0,3,12]\n",
    "p = 0\n",
    "l = []\n",
    "for i in nums:\n",
    "    if i == 0:\n",
    "        l.append(i)\n",
    "        nums.remove(i)\n",
    "nums.extend(l)\n",
    "\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [0,0,1]\n",
    "\n",
    "c = 0\n",
    "\n",
    "for i in range(len(nums)):\n",
    "    if nums[i] !=0:\n",
    "        nums[c] , nums[i] = nums[i], nums[c]\n",
    "        c +=1\n",
    "\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"abc\"\n",
    "t = \"ahbgdc\"\n",
    "count = 0\n",
    "for i in t:\n",
    "    if i in s:\n",
    "        count +=1\n",
    "if count >= len(s):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"abc\"\n",
    "t = \"ahbgdc\"\n",
    "c = 1\n",
    "for i in range(len(s)):\n",
    "    for j in range(len(t)):\n",
    "        if s[i] ==t[j]:\n",
    "            if c>= i:\n",
    "                c +=1\n",
    "\n",
    "if c>= len(s):\n",
    "    print(\"T\")\n",
    "else:\n",
    "    print(\"F\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen AI Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Open the file and load the JSON content\n",
    "file_path = 'src/input/Dataset for DS Case Study.json'\n",
    "\n",
    "# Open and read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Output the first entry for verification\n",
    "print(data[0])  # Displays the first review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame(data)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Fill NaN values with empty strings\n",
    "df['reviewText'] = df['reviewText'].fillna('')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_reviewText'] = df['reviewText'].apply(clean_text)\n",
    "\n",
    "df[['reviewText', 'cleaned_reviewText']].head(19)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy\n",
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download en_core_web_lg \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"person\",      # people, including fictional characters\n",
    "    \"fac\",         # buildings, airports, highways, bridges\n",
    "    \"org\",         # organizations, companies, agencies, institutions\n",
    "    \"gpe\",         # geopolitical entities like countries, cities, states\n",
    "    \"loc\",         # non-gpe locations\n",
    "    \"product\",     # vehicles, foods, appareal, appliances, software, toys \n",
    "    \"event\",       # named sports, scientific milestones, historical events\n",
    "    \"work_of_art\", # titles of books, songs, movies\n",
    "    \"law\",         # named laws, acts, or legislations\n",
    "    \"language\",    # any named language\n",
    "    \"date\",        # absolute or relative dates or periods\n",
    "    \"time\",        # time units smaller than a day\n",
    "    \"percent\",     # percentage (e.g., \"twenty percent\", \"18%\")\n",
    "    \"money\",       # monetary values, including unit\n",
    "    \"quantity\",    # measurements, e.g., weight or distance\n",
    "    \"automobile\",  # name (e.g., brand or model names)\n",
    "    \"Components\",   #(e.g., engine, tires, steering wheel, etc.)\n",
    "    \n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(free_text): \n",
    "     # Define a prompt to instruct the model to extract entities\n",
    "    prompt = f\"\"\"\n",
    "    Extract all possible named entities from the following text:\n",
    "\n",
    "    Text: \\\"{free_text}\\\"\n",
    "\n",
    "    Entities should include people, organizations, locations, products, and any other relevant categories.\n",
    "    Return the entities in a JSON format with their category (e.g., \"person\", \"organization\", \"location\", \"product\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Make the API call to extract entities\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract all possible named entities from the following text:\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Entities should include {labels}, and any other relevant categories. \"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"\"Return only the entities in a JSON format with their category (e.g., \"person\", \"organization\", \"location\", \"product\"). no extra text , connects of not available keep it null\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{free_text}\"}\n",
    "  ]\n",
    "       \n",
    "    )\n",
    "\n",
    "    # Get the model's response\n",
    "    entities = response.choices[0].message.content\n",
    "    time.sleep(2)\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NER_Tagging'] = df['reviewText'].apply(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json(text):\n",
    "    try:\n",
    "        print(json.loads(text[8:-4]))\n",
    "        return  json.loads(text[8:-4])\n",
    "    except:\n",
    "        print(json.loads(text[:]))\n",
    "        return  json.loads(text[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['JSON_clean'] = df['NER_Tagging'].apply(clean_json)\n",
    "df['JSON_clean'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagging = pd.json_normalize(df['JSON_clean'])\n",
    "\n",
    "merge_pd = pd.merge(df,ner_tagging,left_index=True, right_index=True)\n",
    "\n",
    "merge_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "df['Embeddings'] = df['cleaned_reviewText'].apply(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def recommend_product(review_text,df,top_n=5):\n",
    "    input_embedding = get_embedding (review_text)\n",
    "    similarities = cosine_similarity([input_embedding], list(df['Embeddings']))\n",
    "    top_indices = np.argsort(similarities[0][::-1][:top_n])\n",
    "    return df.iloc[top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"I am planning to buy insurance for my car, can you recommend some best car insurance company?\"\n",
    "recommend_product(text,df)\n",
    "\n",
    "\n",
    "\n",
    "def Recommendation(text): \n",
    "     # Define a prompt to instruct the model to extract entities\n",
    "    prompt = f\"\"\"{text}\"\"\"\n",
    "\n",
    "    # Make the API call to extract entities\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are helpgul assistant to extract insight from json structure\"},\n",
    "            {\"role\": \"user\", \"content\": \"Text is given in below statement in json format extract only product name only \"},\n",
    "            {\"role\": \"assistant\", \"content\": \" statement in json format extract information and restruct in english sentence,only englist statement no additional comments \"},\n",
    "            {\"role\": \"user\", \"content\": f\"{text}\"}\n",
    "  ],\n",
    "  temperature= 0\n",
    "       \n",
    "    )\n",
    "\n",
    "    # Get the model's response\n",
    "    entities = response.choices[0].message.content\n",
    "    time.sleep(2)\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "Recommendation(recommend_product(text,df)['JSON_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = set()\n",
    "for i in recommend_product(text,df)['JSON_clean']:\n",
    "    l.add(Recommendation(i))\n",
    "    time.sleep(2)\n",
    "\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_review = \"Looking for car shiner polish to clean car\"\n",
    "\n",
    "# Get recommendations based on the example review\n",
    "recommendations = recommend_product(example_review, df)\n",
    "\n",
    "# Display recommendations\n",
    "recommendations\n",
    "\n",
    "\n",
    "l = set()\n",
    "for i in recommendations['JSON_clean']:\n",
    "    l.add(Recommendation(i))\n",
    "    time.sleep(2)\n",
    "\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ ={3:\"dd\",2:\"df\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dict_:\n",
    "    if i ==1:\n",
    "        print(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructured library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured[all-docs] in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (0.16.25)\n",
      "Requirement already satisfied: chardet in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (5.2.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (0.4.27)\n",
      "Requirement already satisfied: lxml in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (5.3.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (3.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (4.13.3)\n",
      "Requirement already satisfied: emoji in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (2.14.1)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (2025.2.18)\n",
      "Requirement already satisfied: langdetect in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (1.0.9)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (3.12.2)\n",
      "Requirement already satisfied: backoff in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (4.12.2)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (0.31.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (1.17.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (5.9.0)\n",
      "Requirement already satisfied: python-oxmsg in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (0.0.2)\n",
      "Requirement already satisfied: html5lib in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (1.1)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (1.17.0)\n",
      "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (0.3.15)\n",
      "Requirement already satisfied: python-docx>=1.1.2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (1.1.2)\n",
      "Requirement already satisfied: pypandoc in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (1.15)\n",
      "Requirement already satisfied: pikepdf in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (9.5.2)\n",
      "Requirement already satisfied: effdet in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (0.4.1)\n",
      "Requirement already satisfied: onnx in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (1.17.0)\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (20201018)\n",
      "Requirement already satisfied: unstructured-inference>=0.8.7 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (0.8.9)\n",
      "Requirement already satisfied: google-cloud-vision in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (3.10.0)\n",
      "Requirement already satisfied: python-pptx>=1.0.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (1.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (2.2.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (3.4.2)\n",
      "Requirement already satisfied: markdown in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (3.7)\n",
      "Requirement already satisfied: xlrd in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (2.0.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (3.1.5)\n",
      "Requirement already satisfied: pi-heif in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (0.21.0)\n",
      "Requirement already satisfied: pypdf in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured[all-docs]) (5.3.1)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (11.1.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (3.2.2)\n",
      "Requirement already satisfied: python-multipart in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-inference>=0.8.7->unstructured[all-docs]) (0.0.20)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-inference>=0.8.7->unstructured[all-docs]) (0.29.2)\n",
      "Requirement already satisfied: opencv-python!=4.7.0.68 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-inference>=0.8.7->unstructured[all-docs]) (4.11.0.86)\n",
      "Requirement already satisfied: onnxruntime>=1.17.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-inference>=0.8.7->unstructured[all-docs]) (1.21.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-inference>=0.8.7->unstructured[all-docs]) (3.10.1)\n",
      "Requirement already satisfied: torch in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-inference>=0.8.7->unstructured[all-docs]) (2.6.0)\n",
      "Requirement already satisfied: timm in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-inference>=0.8.7->unstructured[all-docs]) (1.0.15)\n",
      "Requirement already satisfied: transformers>=4.25.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-inference>=0.8.7->unstructured[all-docs]) (4.49.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-inference>=0.8.7->unstructured[all-docs]) (1.15.2)\n",
      "Requirement already satisfied: pypdfium2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-inference>=0.8.7->unstructured[all-docs]) (4.30.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured.pytesseract>=0.3.12->unstructured[all-docs]) (24.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from beautifulsoup4->unstructured[all-docs]) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from dataclasses-json->unstructured[all-docs]) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from effdet->unstructured[all-docs]) (0.21.0)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from effdet->unstructured[all-docs]) (2.0.8)\n",
      "Requirement already satisfied: omegaconf>=2.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from effdet->unstructured[all-docs]) (2.3.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (2.24.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from google-cloud-vision->unstructured[all-docs]) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from google-cloud-vision->unstructured[all-docs]) (1.26.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from google-cloud-vision->unstructured[all-docs]) (5.29.3)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from html5lib->unstructured[all-docs]) (1.17.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from html5lib->unstructured[all-docs]) (0.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from nltk->unstructured[all-docs]) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from nltk->unstructured[all-docs]) (2024.11.6)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from openpyxl->unstructured[all-docs]) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from pandas->unstructured[all-docs]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from pandas->unstructured[all-docs]) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from pandas->unstructured[all-docs]) (2025.1)\n",
      "Requirement already satisfied: cryptography in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from pdfminer.six->unstructured[all-docs]) (44.0.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from pdfminer.six->unstructured[all-docs]) (2.4.0)\n",
      "Requirement already satisfied: Deprecated in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from pikepdf->unstructured[all-docs]) (1.2.18)\n",
      "Requirement already satisfied: olefile in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from python-oxmsg->unstructured[all-docs]) (0.47)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from requests->unstructured[all-docs]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from requests->unstructured[all-docs]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from requests->unstructured[all-docs]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from requests->unstructured[all-docs]) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from tqdm->unstructured[all-docs]) (0.4.6)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (24.1.0)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (2.10.6)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (0.4.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from cryptography->pdfminer.six->unstructured[all-docs]) (1.17.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.69.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.71.0rc2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.71.0rc2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (0.14.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (6.0.2)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference>=0.8.7->unstructured[all-docs]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference>=0.8.7->unstructured[all-docs]) (25.2.10)\n",
      "Requirement already satisfied: sympy in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference>=0.8.7->unstructured[all-docs]) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from matplotlib->unstructured-inference>=0.8.7->unstructured[all-docs]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from matplotlib->unstructured-inference>=0.8.7->unstructured[all-docs]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from matplotlib->unstructured-inference>=0.8.7->unstructured[all-docs]) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from matplotlib->unstructured-inference>=0.8.7->unstructured[all-docs]) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from matplotlib->unstructured-inference>=0.8.7->unstructured[all-docs]) (3.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from pydantic>=2.10.3->unstructured-client->unstructured[all-docs]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from pydantic>=2.10.3->unstructured-client->unstructured[all-docs]) (2.27.2)\n",
      "Requirement already satisfied: safetensors in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from timm->unstructured-inference>=0.8.7->unstructured[all-docs]) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from torch->unstructured-inference>=0.8.7->unstructured[all-docs]) (3.17.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from torch->unstructured-inference>=0.8.7->unstructured[all-docs]) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from torch->unstructured-inference>=0.8.7->unstructured[all-docs]) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference>=0.8.7->unstructured[all-docs]) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from transformers>=4.25.1->unstructured-inference>=0.8.7->unstructured[all-docs]) (0.21.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all-docs]) (1.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from cffi>=1.12->cryptography->pdfminer.six->unstructured[all-docs]) (2.22)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference>=0.8.7->unstructured[all-docs]) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from jinja2->torch->unstructured-inference>=0.8.7->unstructured[all-docs]) (3.0.2)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.17.0->unstructured-inference>=0.8.7->unstructured[all-docs]) (3.5.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uq chromadb tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uq langchain langchain-community langchain-openai langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Using cached onnxruntime-1.21.0-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting numpy>=1.21.6 (from onnxruntime)\n",
      "  Using cached numpy-2.2.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\ai\\anaconda3\\envs\\conda_env\\lib\\site-packages (from onnxruntime) (24.2)\n",
      "Collecting protobuf (from onnxruntime)\n",
      "  Using cached protobuf-6.30.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting sympy (from onnxruntime)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached onnxruntime-1.21.0-cp310-cp310-win_amd64.whl (11.8 MB)\n",
      "Using cached numpy-2.2.3-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached protobuf-6.30.0-cp310-abi3-win_amd64.whl (430 kB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: mpmath, flatbuffers, sympy, pyreadline3, protobuf, numpy, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-25.2.10 humanfriendly-10.0 mpmath-1.3.0 numpy-2.2.3 onnxruntime-1.21.0 protobuf-6.30.0 pyreadline3-3.5.4 sympy-1.13.3\n"
     ]
    }
   ],
   "source": [
    "! pip install onnxruntime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting frontend\n",
      "  Downloading frontend-0.0.3-py3-none-any.whl.metadata (847 bytes)\n",
      "Collecting starlette>=0.12.0 (from frontend)\n",
      "  Using cached starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting uvicorn>=0.7.1 (from frontend)\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting itsdangerous>=1.1.0 (from frontend)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: aiofiles in c:\\users\\ai\\onedrive\\programm\\github\\hf_projects\\pdfenv\\lib\\site-packages (from frontend) (24.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\ai\\onedrive\\programm\\github\\hf_projects\\pdfenv\\lib\\site-packages (from starlette>=0.12.0->frontend) (4.8.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\ai\\onedrive\\programm\\github\\hf_projects\\pdfenv\\lib\\site-packages (from uvicorn>=0.7.1->frontend) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\ai\\onedrive\\programm\\github\\hf_projects\\pdfenv\\lib\\site-packages (from uvicorn>=0.7.1->frontend) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in c:\\users\\ai\\onedrive\\programm\\github\\hf_projects\\pdfenv\\lib\\site-packages (from uvicorn>=0.7.1->frontend) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ai\\onedrive\\programm\\github\\hf_projects\\pdfenv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ai\\onedrive\\programm\\github\\hf_projects\\pdfenv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ai\\onedrive\\programm\\github\\hf_projects\\pdfenv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ai\\onedrive\\programm\\github\\hf_projects\\pdfenv\\lib\\site-packages (from click>=7.0->uvicorn>=0.7.1->frontend) (0.4.6)\n",
      "Downloading frontend-0.0.3-py3-none-any.whl (32 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: itsdangerous, uvicorn, starlette, frontend\n",
      "Successfully installed frontend-0.0.3 itsdangerous-2.2.0 starlette-0.46.1 uvicorn-0.34.0\n"
     ]
    }
   ],
   "source": [
    "# ! pip install fitz\n",
    "! pip install frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "\n",
    "# import nltk \n",
    "\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'src/input/The_Story_of_Gandhi-1-15.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ai\\anaconda3\\envs\\conda_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PSBaseParserToken' from 'pdfminer.psparser' (c:\\Users\\Ai\\anaconda3\\envs\\conda_env\\lib\\site-packages\\pdfminer\\psparser.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_pdf\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Returns a List[Element] present in the pages of the parsed pdf document\u001b[39;00m\n\u001b[0;32m      4\u001b[0m elements \u001b[38;5;241m=\u001b[39m partition_pdf(path)\n",
      "File \u001b[1;32mc:\\Users\\Ai\\anaconda3\\envs\\conda_env\\lib\\site-packages\\unstructured\\partition\\pdf.py:99\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     91\u001b[0m     OCR_AGENT_PADDLE,\n\u001b[0;32m     92\u001b[0m     SORT_MODE_BASIC,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m     PartitionStrategy,\n\u001b[0;32m     97\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msorting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m coord_has_valid_points, sort_page_elements\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdfminer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m patch_psparser\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m first, requires_dependencies\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32mc:\\Users\\Ai\\anaconda3\\envs\\conda_env\\lib\\site-packages\\unstructured\\patches\\pdfminer.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple, Union\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfminer\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfminer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpsparser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     END_KEYWORD,\n\u001b[0;32m      7\u001b[0m     KWD,\n\u001b[0;32m      8\u001b[0m     PSEOF,\n\u001b[0;32m      9\u001b[0m     PSBaseParser,\n\u001b[0;32m     10\u001b[0m     PSBaseParserToken,\n\u001b[0;32m     11\u001b[0m     PSKeyword,\n\u001b[0;32m     12\u001b[0m     log,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m factory_seek \u001b[38;5;241m=\u001b[39m PSBaseParser\u001b[38;5;241m.\u001b[39mseek\n\u001b[0;32m     18\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(PSBaseParser\u001b[38;5;241m.\u001b[39mseek)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mseek\u001b[39m(\u001b[38;5;28mself\u001b[39m: PSBaseParser, pos: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PSBaseParserToken' from 'pdfminer.psparser' (c:\\Users\\Ai\\anaconda3\\envs\\conda_env\\lib\\site-packages\\pdfminer\\psparser.py)"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Returns a List[Element] present in the pages of the parsed pdf document\n",
    "elements = partition_pdf(path)\n",
    "\n",
    "# Applies the English and Swedish language pack for ocr. OCR is only applied\n",
    "# if the text is not available in the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Directory 'static/' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m path\n",
      "File \u001b[1;32mc:\\Users\\Ai\\OneDrive\\Programm\\GitHub\\HF_Projects\\pdfenv\\lib\\site-packages\\fitz\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfrontend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ai\\OneDrive\\Programm\\GitHub\\HF_Projects\\pdfenv\\lib\\site-packages\\frontend\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ai\\OneDrive\\Programm\\GitHub\\HF_Projects\\pdfenv\\lib\\site-packages\\frontend\\events\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclipboard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_mixins\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhash_change\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ai\\OneDrive\\Programm\\GitHub\\HF_Projects\\pdfenv\\lib\\site-packages\\frontend\\events\\clipboard.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_mixins\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ClipboardDataMixin\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Event\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClipboardEvent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mClipboardEvent\u001b[39;00m(Event, ClipboardDataMixin):\n",
      "File \u001b[1;32mc:\\Users\\Ai\\OneDrive\\Programm\\GitHub\\HF_Projects\\pdfenv\\lib\\site-packages\\frontend\\dom.py:439\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatcher\n",
      "File \u001b[1;32mc:\\Users\\Ai\\OneDrive\\Programm\\GitHub\\HF_Projects\\pdfenv\\lib\\site-packages\\frontend\\dispatcher.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstarlette\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mendpoints\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WebSocketEndpoint\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstarlette\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebsockets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WebSocket\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, server\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m later_await\n\u001b[0;32m     18\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreact\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Ai\\OneDrive\\Programm\\GitHub\\HF_Projects\\pdfenv\\lib\\site-packages\\frontend\\server.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroute\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m app: Any \u001b[38;5;241m=\u001b[39m Starlette(debug\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mDEBUG)\n\u001b[1;32m---> 24\u001b[0m app\u001b[38;5;241m.\u001b[39mmount(config\u001b[38;5;241m.\u001b[39mSTATIC_ROUTE, \u001b[43mStaticFiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTATIC_DIRECTORY\u001b[49m\u001b[43m)\u001b[49m, name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mSTATIC_NAME)\n\u001b[0;32m     25\u001b[0m app\u001b[38;5;241m.\u001b[39madd_middleware(GZipMiddleware)\n\u001b[0;32m     26\u001b[0m app\u001b[38;5;241m.\u001b[39madd_middleware(\n\u001b[0;32m     27\u001b[0m     CORSMiddleware,\n\u001b[0;32m     28\u001b[0m     allow_origins\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     allow_headers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Ai\\OneDrive\\Programm\\GitHub\\HF_Projects\\pdfenv\\lib\\site-packages\\starlette\\staticfiles.py:56\u001b[0m, in \u001b[0;36mStaticFiles.__init__\u001b[1;34m(self, directory, packages, html, check_dir, follow_symlink)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_symlink \u001b[38;5;241m=\u001b[39m follow_symlink\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_dir \u001b[38;5;129;01mand\u001b[39;00m directory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(directory):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Directory 'static/' does not exist"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "\n",
    "pdf_path = path\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "extracted_data = []\n",
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc[page_num]\n",
    "    \n",
    "    # Extract text with bounding boxes\n",
    "    for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "        if \"lines\" in block:\n",
    "            text = \" \".join([span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]])\n",
    "            bbox = block[\"bbox\"]  # (x0, y0, x1, y1)\n",
    "            extracted_data.append({\"type\": \"text\", \"content\": text, \"page\": page_num, \"bbox\": bbox})\n",
    "\n",
    "    # Extract images\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        xref = img[0]\n",
    "        base_image = doc.extract_image(xref)\n",
    "        img_filename = f\"page_{page_num + 1}_img_{img_index}.png\"\n",
    "        \n",
    "        with open(img_filename, \"wb\") as img_file:\n",
    "            img_file.write(base_image[\"image\"])\n",
    "        \n",
    "        extracted_data.append({\"type\": \"image\", \"content\": img_filename, \"page\": page_num, \"bbox\": (0, 0, page.rect.width, page.rect.height)})\n",
    "\n",
    "    # Extract tables using Camelot\n",
    "    try:\n",
    "        tables = camelot.read_pdf(pdf_path, pages=str(page_num+1), flavor=\"stream\")\n",
    "        for table in tables:\n",
    "            extracted_data.append({\"type\": \"table\", \"content\": table.df.to_dict(), \"page\": page_num, \"bbox\": (0, 0, page.rect.width, page.rect.height)})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Save extracted metadata\n",
    "with open(\"extracted_data.json\", \"w\") as f:\n",
    "    json.dump(extracted_data, f, indent=4)\n",
    "\n",
    "print(\"Extraction complete with contextual metadata!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
